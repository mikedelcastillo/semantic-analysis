{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-25 05:29:43\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import os.path\n",
    "import gc\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_time():\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "\n",
    "gc.collect()\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-25 05:29:43\n",
      "2018-11-25 05:29:46\n"
     ]
    }
   ],
   "source": [
    "print_time()\n",
    "# Select whether using Keras with or without GPU support\n",
    "# See: https://stackoverflow.com/questions/40690598/can-keras-with-tensorflow-backend-be-forced-to-use-cpu-or-gpu-at-will\n",
    "use_gpu = True\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        allow_soft_placement=True, \n",
    "                        device_count = {'CPU' : 1, \n",
    "                                        'GPU' : 1 if use_gpu else 0})\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-25 06:16:26\n",
      "2018-11-25 06:16:36\n",
      "2040155\n",
      "2040154\n",
      "1868325\n",
      "1868325\n",
      "208455\n"
     ]
    }
   ],
   "source": [
    "print_time()\n",
    "data = pd.read_json('data/processed.json')\n",
    "print_time()\n",
    "print(len(data))\n",
    "data = data.dropna()\n",
    "print(len(data))\n",
    "data = data.drop_duplicates()\n",
    "print(len(data))\n",
    "data = data[data[0] != '']\n",
    "print(len(data))\n",
    "data = data[data[3] != 'word-list']\n",
    "data = data[data[3] != 'standford']\n",
    "data = data[data[3] != 'imdb']\n",
    "data = data[data[3] != 'reviews']\n",
    "data = data[data[3] != 'twitter-airline-sentiment']\n",
    "data = data[data[3] != 'tweets']\n",
    "# data = data[data[3] == 'sentiment-analysis-dataset']\n",
    "data = data[data[3] == 'stanford']\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    116603\n",
       "1.0     91852\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    133070\n",
       "1.0     75385\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of stanford is 208455\n",
      "Positive 0.44063227075387973\n",
      "Negative 0.36163680410640187\n",
      "Text max sitting in the third row of the imax cinema at sydney s darling harbour  but i sometimes felt as though i was in the tiny two seater plane that carried the giant camera around australia  sweeping and gliding  banking and hovering over some of the most not\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = dict(data[3].value_counts())\n",
    "\n",
    "for k in counts:\n",
    "    filtered = data[data[3] == k]\n",
    "    total = np.int(counts[k])\n",
    "    print('Total of {} is {}'.format(k, total))\n",
    "    print('Positive {}'.format(len(filtered[filtered[1] == 1])/total))  \n",
    "    print('Negative {}'.format(len(filtered[filtered[2] == 1])/total))\n",
    "    print('Text max {}'.format(max([x[0] for x in np.array(filtered[[0]])], key=len)))   \n",
    "\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-25 06:17:41\n",
      "Corpus size: 208455\n",
      "2018-11-25 06:17:45\n"
     ]
    }
   ],
   "source": [
    "print_time()\n",
    "corpus = [text_to_word_sequence(y) for y in [x[0] for x in data[[0]].values]]\n",
    "labels = [np.array(x[[0, 1]]) for x in data[[1, 2]].values]\n",
    "    \n",
    "print('Corpus size: {}'.format(len(corpus)))\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-25 05:30:23\n",
      "Loading...\n",
      "2018-11-25 05:31:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "print_time()\n",
    "# Gensim Word2Vec model\n",
    "vector_size = 300\n",
    "window_size = 10\n",
    "\n",
    "# word2vec_name = 'w2v.bin'\n",
    "word2vec_name = 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec = None\n",
    "\n",
    "# Create Word2Vec\n",
    "if os.path.isfile(word2vec_name): \n",
    "    print(\"Loading...\")\n",
    "#     word2vec = Word2Vec.load(word2vec_name)\n",
    "    word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "else:\n",
    "    print(\"Computing...\")\n",
    "    word2vec = Word2Vec(sentences=corpus,\n",
    "                        size=vector_size, \n",
    "                        window=window_size, \n",
    "                        negative=20,\n",
    "                        iter=50,\n",
    "                        seed=1000,\n",
    "                        workers=multiprocessing.cpu_count())\n",
    "    word2vec.save(word2vec_name)\n",
    "\n",
    "# Take vectors of tokens and discard \n",
    "vecs_x = word2vec.wv\n",
    "del word2vec\n",
    "\n",
    "gc.collect()\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_mike(text):\n",
    "    TLGRM_SECRET = '747320373:AAFGP2XI3OtGJ-CqE7z41RRnrR_gq00jeMM'\n",
    "    TLGRM_MIKE = '248923795'\n",
    "    TLGRM_URL = 'http://api.telegram.org/bot' + TLGRM_SECRET + '/sendmessage?chat_id=' + TLGRM_MIKE + '&text=' \n",
    "\n",
    "    import urllib.request\n",
    "    from urllib.parse import quote\n",
    "    contents = urllib.request.urlopen(TLGRM_URL + quote(text, safe='')).read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching tokens with vectors\n",
    "max_sent_length = 35\n",
    "\n",
    "def pad_vec_data(corpus):\n",
    "    gc.collect()\n",
    "    input_matrix = np.zeros((len(corpus), max_sent_length, vector_size), dtype=K.floatx())\n",
    "    for i in range(len(corpus)):\n",
    "        for t, token in enumerate(corpus[i]):\n",
    "            if t >= max_sent_length:\n",
    "                break\n",
    "            if token not in vecs_x:\n",
    "                continue\n",
    "            input_matrix[i, t, :] = vecs_x[token]\n",
    "    return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras convolutional model\n",
    "gc.collect()\n",
    "batch_size = 32\n",
    "nb_epochs = 20\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', input_shape=(max_sent_length, vector_size)))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "def compile_model():\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentences():\n",
    "    mess = [\n",
    "        \"hello there, my name is mike\",\n",
    "        \"what the fuck haha\",\n",
    "        \"i don't fucking like you man\",\n",
    "        \"considering how the usual process for this is tedious, having an app like this solves a lot of problems\",\n",
    "        \"thank you for this\",\n",
    "        \"this is amazing\",\n",
    "        \"this is nothing short of amazing, I cannot believe it\",\n",
    "        \"how are you\",\n",
    "        \"i really like you\",\n",
    "        \"i don't actually like this product!\",\n",
    "        \"it's useless if you can't use it!\",\n",
    "        \"i can't believe i never heard of this\",\n",
    "        \"this is awesome, didn't know I needed this\",\n",
    "        \"so what am i supposed to use this for?\",\n",
    "        \"what do I need this for?\",\n",
    "        \"this is good\",\n",
    "        \"this is not good\",\n",
    "        \"although the movie was great, it lacked impact\",\n",
    "        \"the movie wasnt that nice\",\n",
    "        \"the movie was nice\",\n",
    "        \"this is not acceptable, I lost everything using your app\",\n",
    "        \"that was kinda stupid\",\n",
    "        \"the instructions were unlear and is not friendly for non-techy people\",\n",
    "        \"this is really useful i would definitely tell everyone about it\",\n",
    "        \"i need to try this!\"\n",
    "    ]\n",
    "    pred = model.predict(pad_vec_data(mess))\n",
    "    output = ''\n",
    "\n",
    "    for i ,m in enumerate(mess):\n",
    "        output += ('{} {} {}\\n'.format('POSITIVE:' if pred[i][0] > 0.5 else 'NEGATIVE:', m, pred[i]))\n",
    "        \n",
    "    message_mike(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy_all():\n",
    "    gc.collect()\n",
    "    scores = []\n",
    "    train_batch_size = 1024\n",
    "    total_batches = ceil(len(corpus) / train_batch_size)\n",
    "\n",
    "    for index in range(0, total_batches):\n",
    "            \n",
    "        print_time()\n",
    "        start = index * train_batch_size\n",
    "        end = min(len(corpus), start + train_batch_size - 1)\n",
    "\n",
    "        corpus_batch = corpus[start:end]\n",
    "        label_batch = labels[start:end]\n",
    "        \n",
    "        corpus_pad_vec_data = pad_vec_data(corpus_batch)\n",
    "        pred = model.predict(corpus_pad_vec_data)\n",
    "        \n",
    "        for i, text in enumerate(pred):\n",
    "            label = labels[i]\n",
    "            scores.append(1 if round(pred[0][0]) == label[0] else 0)\n",
    "            clear_output()\n",
    "            print(np.mean(scores), i + start + 1, len(corpus))\n",
    "            print('Batch {} of {}'.format((index + 1), total_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"keras-model.h5\"\n",
    "\n",
    "# https://gist.github.com/Hironsan/e041d6606164bc14c50aa56b989c5fc0\n",
    "\n",
    "def batch_iter(data, labels, batch_size, shuffle=False):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                X, y = pad_vec_data(shuffled_data[start_index: end_index]), shuffled_labels[start_index: end_index]\n",
    "                yield X, y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "    \n",
    "def batch_train_model():\n",
    "    batch_size = 100\n",
    "    train_x, test_x, train_y, test_y = train_test_split(corpus, labels)\n",
    "    train_x = np.array(train_x)\n",
    "    test_x = np.array(test_x)\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "    gc.collect()\n",
    "    \n",
    "    train_steps, train_batches = batch_iter(train_x, train_y, batch_size)\n",
    "    valid_steps, valid_batches = batch_iter(test_x, test_y, batch_size)\n",
    "    \n",
    "    model.fit_generator(\n",
    "        train_batches, \n",
    "        train_steps, \n",
    "        epochs=1, \n",
    "        validation_data=valid_batches, \n",
    "        validation_steps=valid_steps)\n",
    "\n",
    "# Run multiple batches\n",
    "def train_model():\n",
    "    train_batch_size = 10000\n",
    "    total_batches = ceil(len(corpus) / train_batch_size)\n",
    "\n",
    "    for index in range(0, total_batches):\n",
    "        clear_output()\n",
    "        print_time()\n",
    "        message_mike('Batch {} of {}'.format((index + 1), total_batches))\n",
    "        start = index * train_batch_size\n",
    "        end = min(len(corpus), start + train_batch_size - 1)\n",
    "\n",
    "        corpus_batch = corpus[start:end]\n",
    "        label_batch = labels[start:end]\n",
    "\n",
    "        corpus_pad_vec_data = pad_vec_data(corpus_batch)\n",
    "        gc.collect()\n",
    "\n",
    "        train_x, test_x, train_y, test_y = train_test_split(corpus_pad_vec_data, label_batch)\n",
    "\n",
    "        train_x = np.array(train_x)\n",
    "        test_x = np.array(test_x)\n",
    "        train_y = np.array(train_y)\n",
    "        test_y = np.array(test_y)\n",
    "        gc.collect()\n",
    "\n",
    "        history = model.fit(train_x, train_y,\n",
    "                  batch_size=batch_size,\n",
    "                  shuffle=True,\n",
    "                  epochs=nb_epochs,\n",
    "                  validation_data=(test_x, test_y),\n",
    "#                   verbose=0,\n",
    "                  callbacks=[\n",
    "#                       EarlyStopping(min_delta=0.000025, patience=10),\n",
    "                  ])    \n",
    "\n",
    "        message_mike('{}'.format(history.history['acc']))\n",
    "        test_sentences()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model\")\n",
    "    model.save_weights(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-25 06:18:54\n",
      "Batch 2 of 21\n",
      "Train on 7499 samples, validate on 2500 samples\n",
      "Epoch 1/20\n",
      "7499/7499 [==============================] - 3s 351us/step - loss: 0.4038 - acc: 0.7730 - val_loss: 0.3767 - val_acc: 0.7708\n",
      "Epoch 2/20\n",
      "7499/7499 [==============================] - 3s 349us/step - loss: 0.3612 - acc: 0.7950 - val_loss: 0.3662 - val_acc: 0.7864\n",
      "Epoch 3/20\n",
      "7499/7499 [==============================] - 3s 347us/step - loss: 0.3495 - acc: 0.7980 - val_loss: 0.3653 - val_acc: 0.7900\n",
      "Epoch 4/20\n",
      "7499/7499 [==============================] - 3s 352us/step - loss: 0.3404 - acc: 0.7994 - val_loss: 0.3625 - val_acc: 0.8068\n",
      "Epoch 5/20\n",
      "7040/7499 [===========================>..] - ETA: 0s - loss: 0.3331 - acc: 0.8033"
     ]
    }
   ],
   "source": [
    "force_train = True\n",
    "\n",
    "if os.path.isfile(model_file) and not force_train:\n",
    "    print(\"Loading model...\")\n",
    "    model.load_weights(model_file)\n",
    "    compile_model()\n",
    "else:\n",
    "    print(\"Training model...\")\n",
    "    compile_model()\n",
    "    train_model()\n",
    "#     batch_train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
