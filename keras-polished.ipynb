{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import os.path\n",
    "import gc\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def print_time():\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "\n",
    "gc.collect()\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_time()\n",
    "# Select whether using Keras with or without GPU support\n",
    "# See: https://stackoverflow.com/questions/40690598/can-keras-with-tensorflow-backend-be-forced-to-use-cpu-or-gpu-at-will\n",
    "use_gpu = True\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        allow_soft_placement=True, \n",
    "                        device_count = {'CPU' : 1, \n",
    "                                        'GPU' : 1 if use_gpu else 0})\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_time()\n",
    "data = pd.read_json('data/processed.json')\n",
    "offset = 500000\n",
    "length = 1000\n",
    "# data = data[offset:offset+length]\n",
    "print_time()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_time()\n",
    "corpus = [text_to_word_sequence(y) for y in [x[0] for x in data[[0]].values]]\n",
    "labels = [np.array(x[[0, 1]]) for x in data[[1, 2]].values]\n",
    "    \n",
    "print('Corpus size: {}'.format(len(corpus)))\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_time()\n",
    "# Gensim Word2Vec model\n",
    "vector_size = 300\n",
    "window_size = 10\n",
    "\n",
    "word2vec_name = 'w2v.bin'\n",
    "word2vec = None\n",
    "\n",
    "# Create Word2Vec\n",
    "if os.path.isfile(word2vec_name): \n",
    "    print(\"Loading...\")\n",
    "    word2vec = Word2Vec.load(word2vec_name)\n",
    "else:\n",
    "    print(\"Computing...\")\n",
    "    word2vec = Word2Vec(sentences=corpus,\n",
    "                        size=vector_size, \n",
    "                        window=window_size, \n",
    "                        negative=20,\n",
    "                        iter=50,\n",
    "                        seed=1000,\n",
    "                        workers=multiprocessing.cpu_count())\n",
    "    word2vec.save(word2vec_name)\n",
    "\n",
    "# Take vectors of tokens and discard \n",
    "vecs_x = word2vec.wv\n",
    "del word2vec\n",
    "\n",
    "gc.collect()\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching tokens with vectors\n",
    "max_sent_length = 35\n",
    "\n",
    "def pad_vec_data(corpus):\n",
    "    gc.collect()\n",
    "    input_matrix = np.zeros((len(corpus), max_sent_length, vector_size), dtype=K.floatx())\n",
    "    for i in range(len(corpus)):\n",
    "        for t, token in enumerate(corpus[i]):\n",
    "            if t >= max_sent_length:\n",
    "                break\n",
    "            if token not in vecs_x:\n",
    "                continue\n",
    "            input_matrix[i, t, :] = vecs_x[token]\n",
    "    return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras convolutional model\n",
    "batch_size = 32\n",
    "nb_epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', input_shape=(max_sent_length, vector_size)))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "def compile_model():\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"keras-model.h5\"\n",
    "\n",
    "# Run multiple batches\n",
    "def train_model():\n",
    "    train_batch_size = 10000\n",
    "    total_batches = ceil(len(corpus) / train_batch_size)\n",
    "\n",
    "    for index in range(0, total_batches):\n",
    "        clear_output()\n",
    "        print_time()\n",
    "        print('Batch {} of {}'.format((index + 1), total_batches))\n",
    "        start = index * train_batch_size\n",
    "        end = min(len(corpus), start + train_batch_size - 1)\n",
    "\n",
    "        corpus_batch = corpus[start:end]\n",
    "        label_batch = labels[start:end]\n",
    "\n",
    "        corpus_pad_vec_data = pad_vec_data(corpus_batch)\n",
    "        gc.collect()\n",
    "\n",
    "        train_x, test_x, train_y, test_y = train_test_split(corpus_pad_vec_data, label_batch)\n",
    "\n",
    "        train_x = np.array(train_x)\n",
    "        test_x = np.array(test_x)\n",
    "        train_y = np.array(train_y)\n",
    "        test_y = np.array(test_y)\n",
    "        gc.collect()\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(train_x, train_y,\n",
    "                  batch_size=batch_size,\n",
    "                  shuffle=True,\n",
    "                  epochs=nb_epochs,\n",
    "                  validation_data=(test_x, test_y),\n",
    "#                   verbose=0,\n",
    "                  callbacks=[\n",
    "#                       EarlyStopping(min_delta=0.000025, patience=10),\n",
    "                  ])\n",
    "\n",
    "        gc.collect()\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model\")\n",
    "    model.save_weights(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_train = True\n",
    "\n",
    "if os.path.isfile(model_file) and not force_train:\n",
    "    print(\"Loading model...\")\n",
    "    model.load_weights(model_file)\n",
    "    compile_model()\n",
    "else:\n",
    "    print(\"Training model...\")\n",
    "    compile_model()\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = [\n",
    "    \"hello there, my name is mike\",\n",
    "    \"what the fuck haha\",\n",
    "    \"i don't fucking like you man\",\n",
    "    \"don't do this to me dude\",\n",
    "    \"i really like you\",\n",
    "    \"i don't actually like this product!\",\n",
    "    \"it's useless if you can't use it!\",\n",
    "    \"this is good!\",\n",
    "    \"this is not good\",\n",
    "]\n",
    "pred = model.predict(pad_vec_data(mess))\n",
    "\n",
    "for i ,m in enumerate(mess):\n",
    "    print(pred[i], m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
