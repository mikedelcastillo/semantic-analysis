{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/giuseppebonaccorso/061fca8d0dfc6873619efd8f364bfe89\n",
    "\n",
    "import keras.backend as K\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import floor\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "\n",
    "# Set random seed (for reproducibility)\n",
    "np.random.seed(1000)\n",
    "\n",
    "# Select whether using Keras with or without GPU support\n",
    "# See: https://stackoverflow.com/questions/40690598/can-keras-with-tensorflow-backend-be-forced-to-use-cpu-or-gpu-at-will\n",
    "use_gpu = True\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        allow_soft_placement=True, \n",
    "                        device_count = {'CPU' : 1, \n",
    "                                        'GPU' : 1 if use_gpu else 0})\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "# dataset_location = '/twitter/dataset.csv'\n",
    "# model_location = '/twitter/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>easedaman they wiggle independently too much f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200001</th>\n",
       "      <td>easedaman well if u had my life u would see ho...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>easegill good point  in meantime if already lo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>easegill you wont be looking for it on kiwi tv...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200004</th>\n",
       "      <td>easidream hi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0    1    2\n",
       "200000  easedaman they wiggle independently too much f...  0.0  1.0\n",
       "200001  easedaman well if u had my life u would see ho...  1.0  0.0\n",
       "200002  easegill good point  in meantime if already lo...  1.0  0.0\n",
       "200003  easegill you wont be looking for it on kiwi tv...  1.0  0.0\n",
       "200004                                       easidream hi  0.0  1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('data/processed.json')\n",
    "data = data[200000:200100]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 100\n"
     ]
    }
   ],
   "source": [
    "corpus = [x[0] for x in data[[0]].values]\n",
    "labels = [x[[0, 1]] for x in data[[1, 2]].values]\n",
    "    \n",
    "print('Corpus size: {}'.format(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['easedam', 'they', 'wiggl', 'independ', 'too', 'much', 'fri', 'chick'],\n",
       " ['easedam',\n",
       "  'wel',\n",
       "  'if',\n",
       "  'u',\n",
       "  'had',\n",
       "  'my',\n",
       "  'lif',\n",
       "  'u',\n",
       "  'would',\n",
       "  'see',\n",
       "  'how',\n",
       "  'smart',\n",
       "  'mem',\n",
       "  'in',\n",
       "  'miam',\n",
       "  'is'],\n",
       " ['easegil',\n",
       "  'good',\n",
       "  'point',\n",
       "  'in',\n",
       "  'meantim',\n",
       "  'if',\n",
       "  'already',\n",
       "  'log',\n",
       "  'into',\n",
       "  'a',\n",
       "  'googl',\n",
       "  'produc'],\n",
       " ['easegil',\n",
       "  'you',\n",
       "  'wont',\n",
       "  'be',\n",
       "  'look',\n",
       "  'for',\n",
       "  'it',\n",
       "  'on',\n",
       "  'kiw',\n",
       "  'tv',\n",
       "  'someth',\n",
       "  'of',\n",
       "  'a',\n",
       "  'cult',\n",
       "  'follow',\n",
       "  'her',\n",
       "  'al',\n",
       "  'in',\n",
       "  'fun',\n",
       "  'as',\n",
       "  'noon',\n",
       "  'tak',\n",
       "  'it',\n",
       "  'sery',\n",
       "  'her'],\n",
       " ['easidream', 'hi'],\n",
       " ['easierp',\n",
       "  'im',\n",
       "  'going',\n",
       "  'out',\n",
       "  'soon',\n",
       "  'i',\n",
       "  'think',\n",
       "  'i',\n",
       "  'wil',\n",
       "  'tak',\n",
       "  'my',\n",
       "  'sunglass',\n",
       "  'and',\n",
       "  'an',\n",
       "  'umbrell',\n",
       "  'brisbaneweath'],\n",
       " ['easilyamusedtx', 'i', 'cri', 'again', 'see', 'it'],\n",
       " ['easlydstract', 'hes', 'been', 'out'],\n",
       " ['easmart',\n",
       "  'ughhh',\n",
       "  'i',\n",
       "  'cant',\n",
       "  'believ',\n",
       "  'wer',\n",
       "  'stay',\n",
       "  'at',\n",
       "  'diff',\n",
       "  'hotel',\n",
       "  'i',\n",
       "  'want',\n",
       "  'to',\n",
       "  'party',\n",
       "  'with',\n",
       "  'emilyyyy'],\n",
       " ['guysebast',\n",
       "  'geez',\n",
       "  'hop',\n",
       "  'you',\n",
       "  'get',\n",
       "  'them',\n",
       "  'backth',\n",
       "  'on',\n",
       "  'pretty',\n",
       "  'slack',\n",
       "  'cab',\n",
       "  'driv'],\n",
       " ['guysebast', 'hav', 'a', 'good', 'night', 'sleep'],\n",
       " ['guysebast',\n",
       "  'hey',\n",
       "  'heard',\n",
       "  'you',\n",
       "  'got',\n",
       "  'marry',\n",
       "  'pleas',\n",
       "  'reply',\n",
       "  'that',\n",
       "  'would',\n",
       "  'mean',\n",
       "  'soo',\n",
       "  'much',\n",
       "  'x',\n",
       "  'yo',\n",
       "  'biggest',\n",
       "  'fan'],\n",
       " ['guysebast',\n",
       "  'id',\n",
       "  'lik',\n",
       "  'to',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'for',\n",
       "  'introduc',\n",
       "  'us',\n",
       "  'to',\n",
       "  'twit',\n",
       "  'it',\n",
       "  'bril'],\n",
       " ['guysebast', 'oh', 'guuuuuy'],\n",
       " ['guysebast', 'that', 'fab', 'im', 'very', 'proud', 'of', 'you'],\n",
       " ['guysebast',\n",
       "  'you',\n",
       "  'ar',\n",
       "  'strong',\n",
       "  'to',\n",
       "  'get',\n",
       "  'thru',\n",
       "  'that',\n",
       "  'thi',\n",
       "  'planet',\n",
       "  'is',\n",
       "  'a',\n",
       "  'far',\n",
       "  'cry',\n",
       "  'from',\n",
       "  'the',\n",
       "  'harmony',\n",
       "  'habit',\n",
       "  'the',\n",
       "  'cre',\n",
       "  'intend',\n",
       "  'it',\n",
       "  'to',\n",
       "  'be'],\n",
       " ['guysebastianus', 'hi', 'ther', 'welcom', 'to', 'twitterland'],\n",
       " ['guysmith16',\n",
       "  'my',\n",
       "  'famy',\n",
       "  'is',\n",
       "  'from',\n",
       "  'frant',\n",
       "  'but',\n",
       "  'i',\n",
       "  'hav',\n",
       "  'yet',\n",
       "  'to',\n",
       "  'wit',\n",
       "  'the',\n",
       "  '24',\n",
       "  'heur',\n",
       "  'du',\n",
       "  'man',\n",
       "  'which',\n",
       "  'i',\n",
       "  'would',\n",
       "  'lov',\n",
       "  'to',\n",
       "  'do',\n",
       "  'somedaykeep',\n",
       "  'us',\n",
       "  'post'],\n",
       " ['guyt2030',\n",
       "  'oh',\n",
       "  'and',\n",
       "  'btw',\n",
       "  'giv',\n",
       "  'empathy',\n",
       "  'is',\n",
       "  'a',\n",
       "  'correct',\n",
       "  'term',\n",
       "  'httpwwwfamilyresou',\n",
       "  'read',\n",
       "  'mor',\n",
       "  'httpisgdlsmo'],\n",
       " ['guyweb', 'hav', 'a', 'plan', 'dat', 'but', 'not', 'annount', 'it', 'publ'],\n",
       " ['guzenmediajap', 'hav', 'weekday', 'off', 'is', 'gre', 'thing', 'ar', 'op'],\n",
       " ['guzmantwin', 'see', 'you', 'lat'],\n",
       " ['gv', 'but', 'ar', 'you', 'a', 'hippy', 'on', 'acid', 'now', 'lt3'],\n",
       " ['gv',\n",
       "  'and',\n",
       "  'to',\n",
       "  'think',\n",
       "  'cic',\n",
       "  'and',\n",
       "  'cher',\n",
       "  'wer',\n",
       "  'al',\n",
       "  'he',\n",
       "  'had',\n",
       "  'to',\n",
       "  'valid',\n",
       "  'his',\n",
       "  'gre',\n",
       "  'sorry',\n",
       "  'gre',\n",
       "  'pumpkin'],\n",
       " ['gvalentino',\n",
       "  'wow',\n",
       "  'didnt',\n",
       "  'know',\n",
       "  'my',\n",
       "  'gmail',\n",
       "  'account',\n",
       "  'hook',\n",
       "  'in',\n",
       "  'with',\n",
       "  'blog',\n",
       "  'learn',\n",
       "  'sumthin',\n",
       "  'new',\n",
       "  'everyday'],\n",
       " ['gvalentinobal', 'aw', 'u', 'nee', 'us', 'to', 'bring', 'lunch', 'to', 'u'],\n",
       " ['gvalentinobal',\n",
       "  'i',\n",
       "  'am',\n",
       "  'driv',\n",
       "  'my',\n",
       "  'team',\n",
       "  'to',\n",
       "  'boston',\n",
       "  'for',\n",
       "  'tonight',\n",
       "  'launch',\n",
       "  'party'],\n",
       " ['gvegas864',\n",
       "  'hah',\n",
       "  'look',\n",
       "  'at',\n",
       "  'thi',\n",
       "  'boat',\n",
       "  'it',\n",
       "  'would',\n",
       "  'most',\n",
       "  'lik',\n",
       "  'be',\n",
       "  'the',\n",
       "  'day',\n",
       "  'we',\n",
       "  'sel',\n",
       "  'it',\n",
       "  'it',\n",
       "  'was',\n",
       "  'in',\n",
       "  'rough',\n",
       "  'shap'],\n",
       " ['gvegas864',\n",
       "  'i',\n",
       "  'tri',\n",
       "  'doing',\n",
       "  'thi',\n",
       "  'the',\n",
       "  'oth',\n",
       "  'day',\n",
       "  'bt',\n",
       "  'it',\n",
       "  'didnt',\n",
       "  'quiet',\n",
       "  'work',\n",
       "  'out',\n",
       "  'plu',\n",
       "  'i',\n",
       "  'had',\n",
       "  'the',\n",
       "  'headphon',\n",
       "  'pop',\n",
       "  'out',\n",
       "  'every',\n",
       "  'second'],\n",
       " ['gvis', 'i', 'envy', 'you', 'becaus', 'yo', 'going', 'to', 'hawai'],\n",
       " ['gvo', 'so'],\n",
       " ['gvwilson',\n",
       "  'quotunit',\n",
       "  'testingquot',\n",
       "  'isnt',\n",
       "  'high',\n",
       "  'on',\n",
       "  'the',\n",
       "  'list',\n",
       "  'of',\n",
       "  'adob',\n",
       "  'goal',\n",
       "  'for',\n",
       "  'flexbuild',\n",
       "  'and',\n",
       "  'quotscriptabilityquot',\n",
       "  'isnt',\n",
       "  'someth',\n",
       "  'eclips',\n",
       "  'is',\n",
       "  'known',\n",
       "  'for'],\n",
       " ['gw12cats03', 'i', 'lov', 'issu'],\n",
       " ['gw212', 'gab', 'you', 'got', 'twitt'],\n",
       " ['gwan',\n",
       "  'and',\n",
       "  'id',\n",
       "  'go',\n",
       "  'with',\n",
       "  'eith',\n",
       "  'a',\n",
       "  'nos',\n",
       "  'ring',\n",
       "  'or',\n",
       "  'an',\n",
       "  'eyebrow',\n",
       "  'bar',\n",
       "  'in',\n",
       "  'my',\n",
       "  'opin'],\n",
       " ['gwarchol',\n",
       "  'sometim',\n",
       "  'it',\n",
       "  'hard',\n",
       "  'not',\n",
       "  'to',\n",
       "  'feel',\n",
       "  'sorry',\n",
       "  'for',\n",
       "  'you'],\n",
       " ['gwc11713',\n",
       "  'im',\n",
       "  'offend',\n",
       "  'that',\n",
       "  'you',\n",
       "  'think',\n",
       "  'id',\n",
       "  'be',\n",
       "  'dumb',\n",
       "  'enough',\n",
       "  'to',\n",
       "  'do',\n",
       "  'that',\n",
       "  'besid',\n",
       "  'it',\n",
       "  'a',\n",
       "  'bmw',\n",
       "  'thos',\n",
       "  'ar',\n",
       "  'lesa',\n",
       "  'peopl'],\n",
       " ['gwenmitchel',\n",
       "  'what',\n",
       "  'about',\n",
       "  'urb',\n",
       "  'el',\n",
       "  'in',\n",
       "  'a',\n",
       "  'fantasy',\n",
       "  'world',\n",
       "  'orc',\n",
       "  'rid',\n",
       "  'motorcyc'],\n",
       " ['gwenartax', 'im', 'sorry', 'to', 'hear', 'that'],\n",
       " ['gwenartax', 'ye'],\n",
       " ['gwenbel',\n",
       "  'sup',\n",
       "  'is',\n",
       "  'startup',\n",
       "  'princess',\n",
       "  'academy',\n",
       "  'liv',\n",
       "  'ev',\n",
       "  'at',\n",
       "  'disneyland',\n",
       "  'momflu',\n",
       "  'on',\n",
       "  'stag',\n",
       "  'now'],\n",
       " ['gwenbel',\n",
       "  'good',\n",
       "  'to',\n",
       "  'see',\n",
       "  'you',\n",
       "  'in',\n",
       "  'bould',\n",
       "  'today',\n",
       "  'i',\n",
       "  'cant',\n",
       "  'wait',\n",
       "  'to',\n",
       "  'see',\n",
       "  'yo',\n",
       "  'new',\n",
       "  'sit'],\n",
       " ['gwendolynfelton',\n",
       "  'that',\n",
       "  'mak',\n",
       "  'me',\n",
       "  'sad',\n",
       "  'hop',\n",
       "  'she',\n",
       "  'feel',\n",
       "  'bet',\n",
       "  'soon'],\n",
       " ['gweney',\n",
       "  'i',\n",
       "  'am',\n",
       "  'happy',\n",
       "  'with',\n",
       "  'him',\n",
       "  'but',\n",
       "  'he',\n",
       "  'has',\n",
       "  'let',\n",
       "  'me',\n",
       "  'down',\n",
       "  'many',\n",
       "  'tim'],\n",
       " ['gweney', 'hey', 'her', 'a', 'tweet'],\n",
       " ['gweney', 'itouch', 'new', 'toy'],\n",
       " ['grovegrapevin',\n",
       "  'i',\n",
       "  'lov',\n",
       "  'that',\n",
       "  'shirt',\n",
       "  'exceiv',\n",
       "  'everyon',\n",
       "  'alway',\n",
       "  'think',\n",
       "  'im',\n",
       "  'som',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'drunk',\n",
       "  'barcamp',\n",
       "  'tech',\n",
       "  'coconutgrov'],\n",
       " ['grovesmed',\n",
       "  'im',\n",
       "  'look',\n",
       "  'at',\n",
       "  'it',\n",
       "  'from',\n",
       "  'a',\n",
       "  'sal',\n",
       "  'point',\n",
       "  'of',\n",
       "  'view',\n",
       "  'investig',\n",
       "  'greensp',\n",
       "  'the',\n",
       "  'of',\n",
       "  'jock',\n",
       "  'as',\n",
       "  'ind',\n",
       "  'of',\n",
       "  'econom'],\n",
       " ['grovesmed', 'no', 'on', 'win', 'anym', 'it', 'al', 'about', 'tak', 'part'],\n",
       " ['grovesphoto', 'on', 'the', 'oth', 'coast'],\n",
       " ['grovy', 'thank', 'for', 'the', 'ment'],\n",
       " ['growinggold', 'lol', 'i', 'answ', 'to', 'a', 'quest', 'dont', 'i'],\n",
       " ['growlersworld', 'awesom'],\n",
       " ['growlin',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'for',\n",
       "  'the',\n",
       "  'follow',\n",
       "  'hav',\n",
       "  'a',\n",
       "  'lov',\n",
       "  'weekend'],\n",
       " ['growlin',\n",
       "  'swineflu',\n",
       "  'is',\n",
       "  'now',\n",
       "  'cal',\n",
       "  'mexicanflu',\n",
       "  'becaus',\n",
       "  'it',\n",
       "  'has',\n",
       "  'noth',\n",
       "  'to',\n",
       "  'do',\n",
       "  'with',\n",
       "  'swin'],\n",
       " ['growlin',\n",
       "  'hi',\n",
       "  'thx',\n",
       "  'for',\n",
       "  'follow',\n",
       "  'im',\n",
       "  'follow',\n",
       "  'u',\n",
       "  'back',\n",
       "  'i',\n",
       "  'lov',\n",
       "  'jes',\n",
       "  'too',\n",
       "  'idk',\n",
       "  'what',\n",
       "  'to',\n",
       "  'do',\n",
       "  'without',\n",
       "  'him'],\n",
       " ['growlingjo', 'thank', 'aimeelady', 'has', 'bestow', 'it', 'upon', 'me'],\n",
       " ['growlybear',\n",
       "  'i',\n",
       "  'bought',\n",
       "  'the',\n",
       "  'newfangl',\n",
       "  'typ',\n",
       "  'with',\n",
       "  'lay',\n",
       "  'of',\n",
       "  'chewy',\n",
       "  'jel',\n",
       "  'in',\n",
       "  'between',\n",
       "  'the',\n",
       "  'flak'],\n",
       " ['growlybear',\n",
       "  'my',\n",
       "  'wif',\n",
       "  'get',\n",
       "  'jeal',\n",
       "  'ov',\n",
       "  'almost',\n",
       "  'everyth',\n",
       "  'not',\n",
       "  'sur',\n",
       "  'why',\n",
       "  'though'],\n",
       " ['growlybear',\n",
       "  'wel',\n",
       "  'it',\n",
       "  'doe',\n",
       "  'transport',\n",
       "  'money',\n",
       "  'away',\n",
       "  'from',\n",
       "  'yo',\n",
       "  'cashcard',\n",
       "  'who',\n",
       "  'know',\n",
       "  'what',\n",
       "  'the',\n",
       "  'fut',\n",
       "  'wil',\n",
       "  'bring'],\n",
       " ['growmap',\n",
       "  'is',\n",
       "  'yo',\n",
       "  'contract',\n",
       "  'w',\n",
       "  'hugh',\n",
       "  'network',\n",
       "  'for',\n",
       "  'much',\n",
       "  'long',\n",
       "  'that',\n",
       "  'must',\n",
       "  'be',\n",
       "  'annoy'],\n",
       " ['growm', 'it', 'not', 'going', 'very', 'fast'],\n",
       " ['grownmulababy', 'but', 'u', 'dont', 'be', 'fool', 'with', 'me'],\n",
       " ['growninmyheart',\n",
       "  'went',\n",
       "  'through',\n",
       "  'a',\n",
       "  'few',\n",
       "  'mor',\n",
       "  'fail',\n",
       "  'infertil',\n",
       "  'treatmentshow',\n",
       "  'that',\n",
       "  'to',\n",
       "  'ad',\n",
       "  'to',\n",
       "  'the',\n",
       "  'emot',\n",
       "  'rollercoast'],\n",
       " ['growthsolv', 'gr8', 'glad', 'thing', 'r', 'fantast', 'in', 'florid'],\n",
       " ['growwear',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'i',\n",
       "  'real',\n",
       "  'apprecy',\n",
       "  'that',\n",
       "  'you',\n",
       "  'hav',\n",
       "  'inspir',\n",
       "  'me',\n",
       "  'many',\n",
       "  'tim',\n",
       "  'as',\n",
       "  'wel'],\n",
       " ['growwear', 'hey', 'you'],\n",
       " ['growwear', 'mak', 'that', 'loooooonnnnng', 'tim', 'ago'],\n",
       " ['growwear',\n",
       "  'mim',\n",
       "  'hav',\n",
       "  'you',\n",
       "  'been',\n",
       "  'up',\n",
       "  'al',\n",
       "  'night',\n",
       "  'lens',\n",
       "  'again'],\n",
       " ['growwear', 'wellth', 'as', 'and', 'clint', 'eastwood'],\n",
       " ['growwear', 'which', 'ev', 'you', 'pref', 'il', 'leav', 'it', 'to', 'you'],\n",
       " ['growwear',\n",
       "  'yep',\n",
       "  'tomorrow',\n",
       "  'im',\n",
       "  'hav',\n",
       "  'work',\n",
       "  'day',\n",
       "  'and',\n",
       "  'afternoon',\n",
       "  'and',\n",
       "  'ev',\n",
       "  'celebr',\n",
       "  'at',\n",
       "  'firm',\n",
       "  'so',\n",
       "  'off',\n",
       "  'al',\n",
       "  'day',\n",
       "  'oblig',\n",
       "  'to',\n",
       "  'com'],\n",
       " ['growwear', 'yo', 'quit', 'welcom', 'and', 'deserv'],\n",
       " ['grrowl', 'sadfac', 'i', 'hav', 'the', 'sam', 'predica'],\n",
       " ['grrrieebsss', 'yayyyyy', 'that', 'good', 'def', 'his', 'loss', 'kben'],\n",
       " ['grrrlpartsdjs', 'for', 'tonight', 'yay', 'und', 'what', 'nam'],\n",
       " ['grrrlromeo',\n",
       "  'should',\n",
       "  'hav',\n",
       "  'told',\n",
       "  'her',\n",
       "  'you',\n",
       "  'wer',\n",
       "  'that',\n",
       "  'easy',\n",
       "  'eith',\n",
       "  'but',\n",
       "  'perhap',\n",
       "  'im',\n",
       "  'wrong'],\n",
       " ['grrrlromeo',\n",
       "  'stil',\n",
       "  'shy',\n",
       "  'as',\n",
       "  'hel',\n",
       "  'though',\n",
       "  'mak',\n",
       "  'no',\n",
       "  'sens',\n",
       "  'wer',\n",
       "  'the',\n",
       "  'od',\n",
       "  'in',\n",
       "  'a',\n",
       "  'nerdy',\n",
       "  'man'],\n",
       " ['grrrlromeo', 'damn', 'i', 'stil', 'hav', 'to', 'wait', 'anoth', 'hour'],\n",
       " ['grrrlromeo',\n",
       "  'hear',\n",
       "  'her',\n",
       "  'dismiss',\n",
       "  'it',\n",
       "  'lik',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'robbery',\n",
       "  'gon',\n",
       "  'wrong',\n",
       "  'i',\n",
       "  'felt',\n",
       "  'lik',\n",
       "  'she',\n",
       "  'was',\n",
       "  'stomp',\n",
       "  'on',\n",
       "  'al',\n",
       "  'of',\n",
       "  'us'],\n",
       " ['grrrlromeo', 'i', 'wil', 'mak', 'sur', 'to', 'watch', 'that', 'on'],\n",
       " ['grshane',\n",
       "  'what',\n",
       "  'is',\n",
       "  'wrong',\n",
       "  'with',\n",
       "  'yo',\n",
       "  'should',\n",
       "  'if',\n",
       "  'rot',\n",
       "  'cuff'],\n",
       " ['grshane', 'but', 'chocol', 'is', 'recovery'],\n",
       " ['easportsact', 'gre'],\n",
       " ['eastbaytrib', 'thank', 'bab'],\n",
       " ['eastcoastgamblr', 'dont', 'blam', 'you'],\n",
       " ['eastcoastgamblr', 'felt', 'lik', 'a', 'chang'],\n",
       " ['eastcoaststeff', 'that', 'suck'],\n",
       " ['eastdallas',\n",
       "  'earthxpl',\n",
       "  'has',\n",
       "  'anyon',\n",
       "  'ev',\n",
       "  'told',\n",
       "  'ya',\n",
       "  'u',\n",
       "  'smil',\n",
       "  'lik',\n",
       "  'john',\n",
       "  'rit',\n",
       "  'i',\n",
       "  'hav',\n",
       "  'not',\n",
       "  'heard',\n",
       "  'that'],\n",
       " ['eastdallas',\n",
       "  'i',\n",
       "  'couldnt',\n",
       "  'get',\n",
       "  'stat',\n",
       "  'thing',\n",
       "  'print',\n",
       "  'to',\n",
       "  'work',\n",
       "  'im',\n",
       "  'techno',\n",
       "  'illit'],\n",
       " ['eastdallas', 'thanx'],\n",
       " ['eastdallas', 'yo', 'stil', 'up', 'too'],\n",
       " ['eastenderstv', 'i', 'think', 'dotty', 'lov', 'dot', 'real'],\n",
       " ['eastercampsou', 'tent', 'howev'],\n",
       " ['eastermoon', 'thought', 'you', 'might', 'how', 'the', 'parrot'],\n",
       " ['easternmax', 'oh', 'no', 'im', 'so', 'sorry'],\n",
       " ['eastofnor',\n",
       "  'il',\n",
       "  'bring',\n",
       "  'along',\n",
       "  'a',\n",
       "  'sign',\n",
       "  'postcard',\n",
       "  'for',\n",
       "  'you',\n",
       "  'but',\n",
       "  'i',\n",
       "  'hav',\n",
       "  'a',\n",
       "  'feel',\n",
       "  'our',\n",
       "  'path',\n",
       "  'wil',\n",
       "  'cross',\n",
       "  'again'],\n",
       " ['eastofthesunart', 'awww', 'lol', 'cut', 'had', 'a', 'giggl', 'read', 'it'],\n",
       " ['eastofthesunart',\n",
       "  'hov',\n",
       "  'ov',\n",
       "  'an',\n",
       "  'avat',\n",
       "  'you',\n",
       "  'shuld',\n",
       "  'see',\n",
       "  'symbol',\n",
       "  'the',\n",
       "  'top',\n",
       "  'left',\n",
       "  'on',\n",
       "  'arrow',\n",
       "  'going',\n",
       "  'left',\n",
       "  'is',\n",
       "  'reply'],\n",
       " ['eastrees82',\n",
       "  'i',\n",
       "  'know',\n",
       "  'how',\n",
       "  'u',\n",
       "  'feel',\n",
       "  'vicky',\n",
       "  'near',\n",
       "  'didnt',\n",
       "  'get',\n",
       "  'myself',\n",
       "  'thi',\n",
       "  'morn',\n",
       "  'it',\n",
       "  'too',\n",
       "  'sunny',\n",
       "  'to',\n",
       "  'be',\n",
       "  'stuck',\n",
       "  'in',\n",
       "  'an',\n",
       "  'off',\n",
       "  'today',\n",
       "  'how',\n",
       "  'u',\n",
       "  'been']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and stem\n",
    "tkr = RegexpTokenizer('[a-zA-Z0-9]+')\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "    \n",
    "    for i, tweet in enumerate(corpus):\n",
    "        tokens = [stemmer.stem(t) for t in tkr.tokenize(tweet)]\n",
    "        tokenized_corpus.append(tokens)\n",
    "    \n",
    "    return tokenized_corpus\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus);\n",
    "\n",
    "(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 10.84\n",
      "Max tweet length: 26\n"
     ]
    }
   ],
   "source": [
    "# Gensim Word2Vec model\n",
    "vector_size = 300\n",
    "window_size = 10\n",
    "\n",
    "# Create Word2Vec\n",
    "word2vec = Word2Vec(sentences=tokenized_corpus,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=multiprocessing.cpu_count())\n",
    "\n",
    "# Copy word vectors and delete Word2Vec model  and original corpus to save memory\n",
    "X_vecs = word2vec.wv\n",
    "del word2vec\n",
    "del corpus\n",
    "\n",
    "# Train subset size (0 < size < len(tokenized_corpus))\n",
    "train_size = floor(len(tokenized_corpus) * .9)\n",
    "\n",
    "# Test subset size (0 < size < len(tokenized_corpus) - train_size)\n",
    "test_size = floor(len(tokenized_corpus) * .1)\n",
    "\n",
    "# Compute average and max tweet length\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tokenized_corpus:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tokenized_corpus))))\n",
    "print('Max tweet length: {}'.format(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tweet max length (number of tokens)\n",
    "max_tweet_length = 40\n",
    "\n",
    "X_train = np.zeros((train_size, max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
    "X_test = np.zeros((test_size, max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((test_size, 2), dtype=np.int32)\n",
    "\n",
    "for i in range(train_size + test_size):\n",
    "    for t, token in enumerate(tokenized_corpus[i]):\n",
    "        if t >= max_tweet_length:\n",
    "            break\n",
    "        \n",
    "        if token not in X_vecs:\n",
    "            continue\n",
    "    \n",
    "        if i < train_size:\n",
    "            X_train[i, t, :] = X_vecs[token]\n",
    "        else:\n",
    "            X_test[i - train_size, t, :] = X_vecs[token]\n",
    "            \n",
    "    if i < train_size:\n",
    "#         Y_train[i, :] = [1.0, 0.0] if labels[i] == 0 else [0.0, 1.0]\n",
    "        Y_train[i, :] = labels[i]\n",
    "    else:\n",
    "#         Y_test[i - train_size, :] = [1.0, 0.0] if labels[i] == 0 else [0.0, 1.0]\n",
    "        Y_test[i - train_size, :] = labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/100\n",
      "180000/180000 [==============================] - 76s 420us/step - loss: 0.5538 - acc: 0.7201 - val_loss: 0.5216 - val_acc: 0.7460\n",
      "Epoch 2/100\n",
      "180000/180000 [==============================] - 76s 420us/step - loss: 0.5178 - acc: 0.7463 - val_loss: 0.5048 - val_acc: 0.7521\n",
      "Epoch 3/100\n",
      "180000/180000 [==============================] - 61s 339us/step - loss: 0.5035 - acc: 0.7554 - val_loss: 0.4963 - val_acc: 0.7592\n",
      "Epoch 4/100\n",
      "111392/180000 [=================>............] - ETA: 41s - loss: 0.4945 - acc: 0.7619"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-34d3a3dbaf78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m           validation_data=(X_test, Y_test))\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;31m# ,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#           callbacks=[EarlyStopping(min_delta=0.00025, patience=2)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Keras convolutional model\n",
    "batch_size = 32\n",
    "nb_epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', input_shape=(max_tweet_length, vector_size)))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=nb_epochs,\n",
    "          validation_data=(X_test, Y_test))\n",
    "# ,\n",
    "#           callbacks=[EarlyStopping(min_delta=0.00025, patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_c = tokenize_corpus([\n",
    "    \"hi there my name is mike\",\n",
    "    \"what are you trying to do\",\n",
    "    \"i love meg so much\",\n",
    "    \"i fuck hate this shit man\",\n",
    "    \"okay that wasnt that bad right i mean it was okay\",\n",
    "    \"it was not nice\",\n",
    "    \"it was nice\",\n",
    "    \"that was not good\",\n",
    "    \"that was good\",\n",
    "])\n",
    "\n",
    "def predict(tk_c):\n",
    "    input_matrix = np.zeros((len(tk_c), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "    for i in range(len(tk_c)):\n",
    "        for t, token in enumerate(tk_c[i]):\n",
    "            if t >= max_tweet_length:\n",
    "                break\n",
    "            if token not in X_vecs:\n",
    "                continue\n",
    "            input_matrix[i, t, :] = X_vecs[token]\n",
    "    return model.predict(input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tk_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
