{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# https://gist.github.com/giuseppebonaccorso/061fca8d0dfc6873619efd8f364bfe89\n",
    "\n",
    "import keras.backend as K\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import floor\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "\n",
    "# Set random seed (for reproducibility)\n",
    "np.random.seed(1000)\n",
    "\n",
    "# Select whether using Keras with or without GPU support\n",
    "# See: https://stackoverflow.com/questions/40690598/can-keras-with-tensorflow-backend-be-forced-to-use-cpu-or-gpu-at-will\n",
    "use_gpu = True\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        inter_op_parallelism_threads=multiprocessing.cpu_count(), \n",
    "                        allow_soft_placement=True, \n",
    "                        device_count = {'CPU' : 1, \n",
    "                                        'GPU' : 1 if use_gpu else 0})\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "# dataset_location = '/twitter/dataset.csv'\n",
    "# model_location = '/twitter/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>you are probably asleep right now but its 925 ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentiment-analysis-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200001</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentiment-analysis-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>glad you found something else to do</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentiment-analysis-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>yea it was cool shame you couldnt come</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentiment-analysis-dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200004</th>\n",
       "      <td>working way too latehow about you</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sentiment-analysis-dataset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0    1    2  \\\n",
       "200000  you are probably asleep right now but its 925 ...  1.0  0.0   \n",
       "200001                                             thanks  1.0  0.0   \n",
       "200002                glad you found something else to do  1.0  0.0   \n",
       "200003             yea it was cool shame you couldnt come  1.0  0.0   \n",
       "200004                  working way too latehow about you  1.0  0.0   \n",
       "\n",
       "                                 3  \n",
       "200000  sentiment-analysis-dataset  \n",
       "200001  sentiment-analysis-dataset  \n",
       "200002  sentiment-analysis-dataset  \n",
       "200003  sentiment-analysis-dataset  \n",
       "200004  sentiment-analysis-dataset  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('data/processed.json')\n",
    "data = data[200000:200100]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 100\n"
     ]
    }
   ],
   "source": [
    "corpus = [x[0] for x in data[[0]].values]\n",
    "labels = [x[[0, 1]] for x in data[[1, 2]].values]\n",
    "    \n",
    "print('Corpus size: {}'.format(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['you',\n",
       "  'ar',\n",
       "  'prob',\n",
       "  'asleep',\n",
       "  'right',\n",
       "  'now',\n",
       "  'but',\n",
       "  'it',\n",
       "  '925',\n",
       "  'am',\n",
       "  'and',\n",
       "  'adam',\n",
       "  'lambert',\n",
       "  'is',\n",
       "  'on',\n",
       "  'liv',\n",
       "  'with',\n",
       "  'reg',\n",
       "  'and',\n",
       "  'kel'],\n",
       " ['thank'],\n",
       " ['glad', 'you', 'found', 'someth', 'els', 'to', 'do'],\n",
       " ['ye', 'it', 'was', 'cool', 'sham', 'you', 'couldnt', 'com'],\n",
       " ['work', 'way', 'too', 'latehow', 'about', 'you'],\n",
       " ['yeah'],\n",
       " ['ed', 'that', 'tiny', 'url', 'isnt', 'work', 'boo'],\n",
       " ['going',\n",
       "  'for',\n",
       "  'com',\n",
       "  'con',\n",
       "  'hop',\n",
       "  'the',\n",
       "  'swin',\n",
       "  'flu',\n",
       "  'scar',\n",
       "  'would',\n",
       "  'hav',\n",
       "  'died',\n",
       "  'down',\n",
       "  'by',\n",
       "  'then'],\n",
       " ['get',\n",
       "  '100',\n",
       "  'follow',\n",
       "  'a',\n",
       "  'day',\n",
       "  'us',\n",
       "  'wwwtweeteraddercom',\n",
       "  'ont',\n",
       "  'you',\n",
       "  'ad',\n",
       "  'everyon',\n",
       "  'you',\n",
       "  'ar',\n",
       "  'on',\n",
       "  'the',\n",
       "  'train',\n",
       "  'or',\n",
       "  'pay',\n",
       "  'vip'],\n",
       " ['oh',\n",
       "  'hello',\n",
       "  'good',\n",
       "  'morn',\n",
       "  'yo',\n",
       "  'the',\n",
       "  'first',\n",
       "  'person',\n",
       "  'who',\n",
       "  'mad',\n",
       "  'me',\n",
       "  'smil',\n",
       "  'today'],\n",
       " ['thank', 'for', 'the', 'feedback', 'guy'],\n",
       " ['act', 'ye'],\n",
       " ['gott', 'agr', 'on', 'the', 'cap', 'rip', 'off'],\n",
       " ['omg', 'thank', 'for', 'tel', 'me', 'they', 'wer', 'los', 'anyway', 'thank'],\n",
       " ['that', 'is', 'extrem', 'clev'],\n",
       " ['ar', 'ther', 'many', 'entrepr', 'in', 'bang'],\n",
       " ['er', 'we', 'miss', 'you', 'com', 'back', 'you', 'us'],\n",
       " ['440v',\n",
       "  'what',\n",
       "  'in',\n",
       "  'the',\n",
       "  'heck',\n",
       "  'wer',\n",
       "  'they',\n",
       "  'run',\n",
       "  'cabinet',\n",
       "  'woodshop',\n",
       "  'in',\n",
       "  'a',\n",
       "  'laundrom'],\n",
       " ['wish',\n",
       "  'id',\n",
       "  'had',\n",
       "  'the',\n",
       "  'sens',\n",
       "  'to',\n",
       "  'real',\n",
       "  'i',\n",
       "  'had',\n",
       "  'a',\n",
       "  'big',\n",
       "  'ol',\n",
       "  'red',\n",
       "  'going',\n",
       "  'on',\n",
       "  'last',\n",
       "  'night'],\n",
       " ['my',\n",
       "  'credit',\n",
       "  'card',\n",
       "  'you',\n",
       "  'wer',\n",
       "  'the',\n",
       "  'on',\n",
       "  'who',\n",
       "  'kept',\n",
       "  'throwing',\n",
       "  'yo',\n",
       "  'behind',\n",
       "  'the',\n",
       "  'bar'],\n",
       " ['mayb'],\n",
       " ['when', 'my', 'team', 'put', 'out', 'a', '1'],\n",
       " ['diamond',\n",
       "  'is',\n",
       "  'good',\n",
       "  'it',\n",
       "  'guar',\n",
       "  'you',\n",
       "  'a',\n",
       "  'room',\n",
       "  'ev',\n",
       "  'if',\n",
       "  'you',\n",
       "  'hav',\n",
       "  'book'],\n",
       " ['nic'],\n",
       " ['no', 'fre', 'vert', 'her', 'in', 'germany', 'yet', 'pay', 'on'],\n",
       " ['wel'],\n",
       " ['im', 'wond', 'soo'],\n",
       " ['for',\n",
       "  'you',\n",
       "  'i',\n",
       "  'would',\n",
       "  'ev',\n",
       "  'be',\n",
       "  'wil',\n",
       "  'to',\n",
       "  'show',\n",
       "  'up',\n",
       "  'as',\n",
       "  'a',\n",
       "  'target'],\n",
       " ['join', 'the', 'club', 'httpblipfm6tew6'],\n",
       " ['seem',\n",
       "  'lik',\n",
       "  '100',\n",
       "  'to',\n",
       "  'me',\n",
       "  'they',\n",
       "  'nee',\n",
       "  'to',\n",
       "  'do',\n",
       "  'the',\n",
       "  'test',\n",
       "  'dur',\n",
       "  'surv'],\n",
       " ['awww', 'wel', 'thank', 'you', 'for', 'yo', 'support'],\n",
       " ['let', 'u', 'know', '2morrow'],\n",
       " ['itl', 'be', 'fin'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'not',\n",
       "  'receiv',\n",
       "  'yo',\n",
       "  'tweet',\n",
       "  'i',\n",
       "  'just',\n",
       "  'saw',\n",
       "  'them',\n",
       "  'on',\n",
       "  'the',\n",
       "  'internet'],\n",
       " ['ther',\n",
       "  'has',\n",
       "  'got',\n",
       "  'to',\n",
       "  'be',\n",
       "  'an',\n",
       "  'interest',\n",
       "  'story',\n",
       "  'in',\n",
       "  'ther',\n",
       "  'how',\n",
       "  'ya',\n",
       "  'been'],\n",
       " ['thank', 'you'],\n",
       " ['cool',\n",
       "  'i',\n",
       "  'lov',\n",
       "  'mountain',\n",
       "  'dew',\n",
       "  'unfortun',\n",
       "  'you',\n",
       "  'cant',\n",
       "  'get',\n",
       "  'it',\n",
       "  'in',\n",
       "  'england',\n",
       "  'hav',\n",
       "  'you',\n",
       "  'got',\n",
       "  'the',\n",
       "  'new',\n",
       "  'iphon'],\n",
       " ['i',\n",
       "  'fin',\n",
       "  'got',\n",
       "  'it',\n",
       "  'to',\n",
       "  'download',\n",
       "  'but',\n",
       "  'it',\n",
       "  'say',\n",
       "  'the',\n",
       "  'serv',\n",
       "  'is',\n",
       "  'ful'],\n",
       " ['thank',\n",
       "  'man',\n",
       "  'that',\n",
       "  'quit',\n",
       "  'a',\n",
       "  'comply',\n",
       "  'com',\n",
       "  'from',\n",
       "  'you',\n",
       "  'mak',\n",
       "  'sur',\n",
       "  'you',\n",
       "  'let',\n",
       "  'everyon',\n",
       "  'know',\n",
       "  'about',\n",
       "  'it',\n",
       "  'we',\n",
       "  'want',\n",
       "  'to',\n",
       "  'go',\n",
       "  'to',\n",
       "  'e3'],\n",
       " ['im',\n",
       "  'just',\n",
       "  'say',\n",
       "  'from',\n",
       "  'a',\n",
       "  'man',\n",
       "  'perspect',\n",
       "  'i',\n",
       "  'would',\n",
       "  'do',\n",
       "  'that',\n",
       "  'i',\n",
       "  'would',\n",
       "  'nev',\n",
       "  'think',\n",
       "  'of',\n",
       "  'it',\n",
       "  'for',\n",
       "  'myself',\n",
       "  'my',\n",
       "  'bf',\n",
       "  'is',\n",
       "  'gonn',\n",
       "  'hat',\n",
       "  'my',\n",
       "  'short',\n",
       "  'hair'],\n",
       " ['nop'],\n",
       " ['cant', 'wait'],\n",
       " ['i', 'believ', 'that', 'the', 'answ'],\n",
       " ['how', 'rub'],\n",
       " ['it',\n",
       "  'wond',\n",
       "  'isnt',\n",
       "  'it',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'of',\n",
       "  'che',\n",
       "  'in',\n",
       "  'the',\n",
       "  'midst',\n",
       "  'of',\n",
       "  'al',\n",
       "  'thi',\n",
       "  'revid'],\n",
       " ['i', 'nev', 'bought', 'ghsh', 'it', 'was', 'gam'],\n",
       " ['i', 'sur', 'am', 'becaus', 'im', 'green', 'and', 'misunderstood'],\n",
       " ['good',\n",
       "  'luck',\n",
       "  'with',\n",
       "  'that',\n",
       "  'my',\n",
       "  'friend',\n",
       "  'i',\n",
       "  'know',\n",
       "  'it',\n",
       "  'not',\n",
       "  'an',\n",
       "  'easy',\n",
       "  'thing',\n",
       "  'to',\n",
       "  'do'],\n",
       " ['aw', 'miss', 'you', 'toothank', 'for', 'hav', 'us', 'overr'],\n",
       " ['hey'],\n",
       " ['lol',\n",
       "  'dont',\n",
       "  'forget',\n",
       "  'im',\n",
       "  'hidin',\n",
       "  'in',\n",
       "  'ur',\n",
       "  'suitcas',\n",
       "  'for',\n",
       "  'ur',\n",
       "  'trip'],\n",
       " ['lol', 'yeah', 'dont', 'rub', 'it', 'in'],\n",
       " ['noth',\n",
       "  'just',\n",
       "  'lay',\n",
       "  'on',\n",
       "  'my',\n",
       "  'bed',\n",
       "  'cuz',\n",
       "  'i',\n",
       "  'went',\n",
       "  'ic',\n",
       "  'skat',\n",
       "  'and',\n",
       "  'i',\n",
       "  'fel',\n",
       "  'on',\n",
       "  'my',\n",
       "  'but',\n",
       "  'hard',\n",
       "  'lol'],\n",
       " ['you', 'act', 'fail', 'to', 'ment', 'that'],\n",
       " ['i',\n",
       "  'saw',\n",
       "  'rent',\n",
       "  'on',\n",
       "  'broadway',\n",
       "  'in',\n",
       "  '96',\n",
       "  'but',\n",
       "  'rap',\n",
       "  'was',\n",
       "  'sick',\n",
       "  'and',\n",
       "  'ther',\n",
       "  'was',\n",
       "  'an',\n",
       "  'understudy'],\n",
       " ['but',\n",
       "  'i',\n",
       "  'saw',\n",
       "  'u',\n",
       "  'thi',\n",
       "  'morn',\n",
       "  'aww',\n",
       "  'kks',\n",
       "  'wel',\n",
       "  'coach',\n",
       "  'j',\n",
       "  'saw',\n",
       "  'em',\n",
       "  'tel',\n",
       "  'me',\n",
       "  'someth',\n",
       "  'then',\n",
       "  'saw',\n",
       "  'me',\n",
       "  'cry',\n",
       "  'and',\n",
       "  'was',\n",
       "  'lik',\n",
       "  'r',\n",
       "  'u',\n",
       "  'oh',\n",
       "  'k',\n",
       "  'can',\n",
       "  'i',\n",
       "  'help'],\n",
       " ['omg', 'i', 'lov', 'you', 'lif', 'sav'],\n",
       " ['i', 'know'],\n",
       " ['i',\n",
       "  'hear',\n",
       "  'ya',\n",
       "  'my',\n",
       "  'lack',\n",
       "  'of',\n",
       "  'job',\n",
       "  'is',\n",
       "  'the',\n",
       "  'on',\n",
       "  'thing',\n",
       "  'that',\n",
       "  'keep',\n",
       "  'me',\n",
       "  'in',\n",
       "  'the',\n",
       "  'book',\n",
       "  'sometim',\n",
       "  'be',\n",
       "  'sur',\n",
       "  'to',\n",
       "  'tel',\n",
       "  'yo',\n",
       "  'lady',\n",
       "  'friend',\n",
       "  'i',\n",
       "  'said',\n",
       "  'hello'],\n",
       " ['no'],\n",
       " ['look', 'spicy', 'and', 'delicy', 'i', 'hav', 'cook', 'in', 'ag'],\n",
       " ['just',\n",
       "  'for',\n",
       "  'you',\n",
       "  'it',\n",
       "  'not',\n",
       "  'the',\n",
       "  'dai',\n",
       "  'increas',\n",
       "  'but',\n",
       "  'dai',\n",
       "  'decreas',\n",
       "  'hack',\n",
       "  'away',\n",
       "  'at',\n",
       "  'the',\n",
       "  'uness',\n",
       "  'bruc',\n",
       "  'lee'],\n",
       " ['dalla',\n",
       "  'county',\n",
       "  'in',\n",
       "  'texa',\n",
       "  'low',\n",
       "  'spee',\n",
       "  'limit',\n",
       "  'to',\n",
       "  '60',\n",
       "  'from',\n",
       "  '70',\n",
       "  'ev',\n",
       "  'in',\n",
       "  'rel',\n",
       "  'rur',\n",
       "  'area',\n",
       "  'got',\n",
       "  'nail',\n",
       "  'thi',\n",
       "  'weekend'],\n",
       " ['thank', 'you', 'for', 'the', 'info', 'i', 'wil', 'check', 'it', 'out'],\n",
       " ['unl',\n",
       "  'al',\n",
       "  'party',\n",
       "  'u',\n",
       "  'wsh',\n",
       "  'to',\n",
       "  'hv',\n",
       "  'ths',\n",
       "  'cht',\n",
       "  'wth',\n",
       "  'r',\n",
       "  'on',\n",
       "  'xp',\n",
       "  'or',\n",
       "  'oldr',\n",
       "  'thn',\n",
       "  'seem',\n",
       "  'ur',\n",
       "  'out',\n",
       "  'of',\n",
       "  'luck',\n",
       "  'fr',\n",
       "  'netmeet',\n",
       "  'anywy',\n",
       "  'cnt',\n",
       "  'think',\n",
       "  'of',\n",
       "  'anythn',\n",
       "  'els'],\n",
       " ['guess', 'that', 'wont', 'work', 'fail'],\n",
       " ['im', 'right', 'her', 'stuck', 'at', 'work', 'lol', 'httpmylocme41rq'],\n",
       " ['try', 'httpbitlyi51ao', 'for', 'continu', 'upd', 'on', 'yo', 'typ', 'abl'],\n",
       " ['jup'],\n",
       " ['oh'],\n",
       " ['i',\n",
       "  'agr',\n",
       "  'my',\n",
       "  'main',\n",
       "  'wish',\n",
       "  'is',\n",
       "  'that',\n",
       "  'anim',\n",
       "  'be',\n",
       "  'kept',\n",
       "  'in',\n",
       "  'good',\n",
       "  'condit'],\n",
       " ['popery'],\n",
       " ['i',\n",
       "  'know',\n",
       "  'ev',\n",
       "  'though',\n",
       "  'i',\n",
       "  'dont',\n",
       "  'know',\n",
       "  'who',\n",
       "  'yo',\n",
       "  'talk',\n",
       "  'about'],\n",
       " ['oh',\n",
       "  'you',\n",
       "  'think',\n",
       "  'o',\n",
       "  'thank',\n",
       "  'u',\n",
       "  'so',\n",
       "  'much',\n",
       "  'lt3',\n",
       "  'i',\n",
       "  'lov',\n",
       "  'britney',\n",
       "  'lt3'],\n",
       " ['yep',\n",
       "  'brit',\n",
       "  'is',\n",
       "  'com',\n",
       "  'to',\n",
       "  'swed',\n",
       "  'but',\n",
       "  'il',\n",
       "  'see',\n",
       "  'her',\n",
       "  'in',\n",
       "  'denmark',\n",
       "  'i',\n",
       "  'liv',\n",
       "  'clos',\n",
       "  'to',\n",
       "  'park',\n",
       "  'wher',\n",
       "  'shel',\n",
       "  'perform',\n",
       "  'wil',\n",
       "  'u',\n",
       "  'see',\n",
       "  'her'],\n",
       " ['hav',\n",
       "  'u',\n",
       "  'heard',\n",
       "  'about',\n",
       "  'pet',\n",
       "  'facinel',\n",
       "  'iphon',\n",
       "  'apply',\n",
       "  'cool',\n",
       "  'stuff',\n",
       "  'but',\n",
       "  'it',\n",
       "  'not',\n",
       "  'for',\n",
       "  'fre'],\n",
       " ['thank', 'for', 'the', 'tea', 'off'],\n",
       " ['cant', 'wait', 'hot', 'man'],\n",
       " ['wows'],\n",
       " ['you',\n",
       "  'got',\n",
       "  'the',\n",
       "  'compass',\n",
       "  'i',\n",
       "  'didnt',\n",
       "  'get',\n",
       "  'that',\n",
       "  'in',\n",
       "  'my',\n",
       "  'upd'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'but',\n",
       "  'iv',\n",
       "  'been',\n",
       "  'asleep',\n",
       "  'and',\n",
       "  'am',\n",
       "  'now',\n",
       "  'up',\n",
       "  'lol',\n",
       "  'aww',\n",
       "  'how',\n",
       "  'long',\n",
       "  'she',\n",
       "  'in',\n",
       "  'london',\n",
       "  'for',\n",
       "  'why',\n",
       "  'couldnt',\n",
       "  'she',\n",
       "  'sneak',\n",
       "  'you',\n",
       "  'in',\n",
       "  'her',\n",
       "  'bag'],\n",
       " ['thx', 'glad', 'you', 'agr', 'che'],\n",
       " ['how',\n",
       "  'sweet',\n",
       "  'howeverth',\n",
       "  'sleep',\n",
       "  'angel',\n",
       "  'seem',\n",
       "  'to',\n",
       "  'hav',\n",
       "  'been',\n",
       "  'distract',\n",
       "  'on',\n",
       "  'their',\n",
       "  'way',\n",
       "  'just',\n",
       "  'wok',\n",
       "  'up',\n",
       "  'and',\n",
       "  'cant',\n",
       "  'fal',\n",
       "  'back',\n",
       "  'asleep'],\n",
       " ['ahhhh', 'no', 'way'],\n",
       " ['they', 'must', 'hav', 'split', 'as', 'a', 'band', 'boo'],\n",
       " ['13', 'hour', 'ago'],\n",
       " ['awwwwn', 'ide', 'but', 'she', 'is', 'not', 'us', 'twit'],\n",
       " ['try', 'that', 'right', 'now', 'thank', 'and', 'welcom', 'to', 'twit'],\n",
       " ['chang', 'yo', 'layout', 'let', 'me', 'know', 'if', 'it', 'ok'],\n",
       " ['yay',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'hamiy',\n",
       "  'im',\n",
       "  'get',\n",
       "  'bet',\n",
       "  'now',\n",
       "  'my',\n",
       "  'sor',\n",
       "  'throat',\n",
       "  'is',\n",
       "  'almost',\n",
       "  'gon',\n",
       "  'no',\n",
       "  'sign',\n",
       "  'of',\n",
       "  'fev',\n",
       "  'or',\n",
       "  'runny',\n",
       "  'nos',\n",
       "  'or',\n",
       "  'headach'],\n",
       " ['sound', 'as', 'sweet', 'as', 'poss', 'tweet', 'tweet'],\n",
       " ['by', 'the', 'way'],\n",
       " ['i',\n",
       "  'spok',\n",
       "  'i',\n",
       "  'spok',\n",
       "  'to',\n",
       "  'you',\n",
       "  'about',\n",
       "  'adam',\n",
       "  'and',\n",
       "  'ev',\n",
       "  'in',\n",
       "  'the',\n",
       "  'dream',\n",
       "  'too',\n",
       "  'but',\n",
       "  'i',\n",
       "  'had',\n",
       "  'to',\n",
       "  'go',\n",
       "  'as',\n",
       "  'i',\n",
       "  'nee',\n",
       "  'to',\n",
       "  'go',\n",
       "  'to',\n",
       "  'the',\n",
       "  'toilet'],\n",
       " ['i', 'should', 'be', 'so', 'lucky'],\n",
       " ['hah', 'it', 'al', 'plan', 'out'],\n",
       " ['awwww'],\n",
       " ['nic', 'ont', 'iv', 'emerg', 'from', 'my', 'food', 'com'],\n",
       " ['me',\n",
       "  'too',\n",
       "  'dont',\n",
       "  'panic',\n",
       "  'spent',\n",
       "  'a',\n",
       "  'coupl',\n",
       "  'of',\n",
       "  'day',\n",
       "  'panick',\n",
       "  'and',\n",
       "  'i',\n",
       "  'didnt',\n",
       "  'rev',\n",
       "  'at',\n",
       "  'al',\n",
       "  'yo',\n",
       "  'pim'],\n",
       " ['i', 'know', 'you', 'areth', 'mor', 'i', 'think', 'about', 'it'],\n",
       " ['guy']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and stem\n",
    "tkr = RegexpTokenizer('[a-zA-Z0-9]+')\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "    \n",
    "    for i, tweet in enumerate(corpus):\n",
    "        tokens = [stemmer.stem(t) for t in tkr.tokenize(tweet)]\n",
    "        tokenized_corpus.append(tokens)\n",
    "    \n",
    "    return tokenized_corpus\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus);\n",
    "\n",
    "(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 9.69\n",
      "Max tweet length: 30\n"
     ]
    }
   ],
   "source": [
    "# Gensim Word2Vec model\n",
    "vector_size = 300\n",
    "window_size = 10\n",
    "\n",
    "# Create Word2Vec\n",
    "word2vec = Word2Vec(sentences=tokenized_corpus,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=multiprocessing.cpu_count())\n",
    "\n",
    "# Copy word vectors and delete Word2Vec model  and original corpus to save memory\n",
    "X_vecs = word2vec.wv\n",
    "del word2vec\n",
    "del corpus\n",
    "\n",
    "# Train subset size (0 < size < len(tokenized_corpus))\n",
    "train_size = floor(len(tokenized_corpus) * .9)\n",
    "\n",
    "# Test subset size (0 < size < len(tokenized_corpus) - train_size)\n",
    "test_size = floor(len(tokenized_corpus) * .1)\n",
    "\n",
    "# Compute average and max tweet length\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tokenized_corpus:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tokenized_corpus))))\n",
    "print('Max tweet length: {}'.format(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tweet max length (number of tokens)\n",
    "max_tweet_length = 40\n",
    "\n",
    "X_train = np.zeros((train_size, max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
    "X_test = np.zeros((test_size, max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((test_size, 2), dtype=np.int32)\n",
    "\n",
    "for i in range(train_size + test_size):\n",
    "    for t, token in enumerate(tokenized_corpus[i]):\n",
    "        if t >= max_tweet_length:\n",
    "            break\n",
    "        \n",
    "        if token not in X_vecs:\n",
    "            continue\n",
    "    \n",
    "        if i < train_size:\n",
    "            X_train[i, t, :] = X_vecs[token]\n",
    "        else:\n",
    "            X_test[i - train_size, t, :] = X_vecs[token]\n",
    "            \n",
    "    if i < train_size:\n",
    "#         Y_train[i, :] = [1.0, 0.0] if labels[i] == 0 else [0.0, 1.0]\n",
    "        Y_train[i, :] = labels[i]\n",
    "    else:\n",
    "#         Y_test[i - train_size, :] = [1.0, 0.0] if labels[i] == 0 else [0.0, 1.0]\n",
    "        Y_test[i - train_size, :] = labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.07627117,  0.06672327,  0.10996481, ..., -0.04921505,\n",
       "          0.0310328 , -0.00809618],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.09522618,  0.08282398,  0.13988431, ..., -0.06200517,\n",
       "          0.03767472, -0.01032154],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.1162397 ,  0.09999683,  0.16817641, ..., -0.07558712,\n",
       "          0.04839981, -0.01285767],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-0.1162397 ,  0.09999683,  0.16817641, ..., -0.07558712,\n",
       "          0.04839981, -0.01285767],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.08989127,  0.07765105,  0.13128905, ..., -0.05626674,\n",
       "          0.03459438, -0.01177149],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.1162397 ,  0.09999683,  0.16817641, ..., -0.07558712,\n",
       "          0.04839981, -0.01285767],\n",
       "        [-0.0856557 ,  0.07398249,  0.12472975, ..., -0.05662041,\n",
       "          0.03612265, -0.00865578],\n",
       "        [-0.10371096,  0.08856297,  0.15173322, ..., -0.06783224,\n",
       "          0.04344827, -0.01105181],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1d_1/convolution/Conv2D}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv1d_1/convolution/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv1d_1/convolution/ExpandDims_1)]]\n\t [[{{node metrics/acc/Mean/_227}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1798_metrics/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-de7b0215d928>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m           validation_data=(X_test, Y_test))\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;31m# ,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#           callbacks=[EarlyStopping(min_delta=0.00025, patience=2)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1d_1/convolution/Conv2D}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv1d_1/convolution/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv1d_1/convolution/ExpandDims_1)]]\n\t [[{{node metrics/acc/Mean/_227}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1798_metrics/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "# Keras convolutional model\n",
    "batch_size = 32\n",
    "nb_epochs = 20\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same', input_shape=(max_tweet_length, vector_size)))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=2, activation='elu', padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=nb_epochs,\n",
    "          validation_data=(X_test, Y_test))\n",
    "# ,\n",
    "#           callbacks=[EarlyStopping(min_delta=0.00025, patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_c = tokenize_corpus([\n",
    "    \"hi there my name is mike\",\n",
    "    \"what are you trying to do\",\n",
    "    \"i love meg so much\",\n",
    "    \"i fuck hate this shit man\",\n",
    "    \"okay that wasnt that bad right i mean it was okay\",\n",
    "    \"it was not nice\",\n",
    "    \"it was nice\",\n",
    "    \"that was not good\",\n",
    "    \"that was good\",\n",
    "])\n",
    "\n",
    "def predict(tk_c):\n",
    "    input_matrix = np.zeros((len(tk_c), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "    for i in range(len(tk_c)):\n",
    "        for t, token in enumerate(tk_c[i]):\n",
    "            if t >= max_tweet_length:\n",
    "                break\n",
    "            if token not in X_vecs:\n",
    "                continue\n",
    "            input_matrix[i, t, :] = X_vecs[token]\n",
    "    return model.predict(input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(tk_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
